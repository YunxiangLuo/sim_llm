from openai import OpenAI
import rclpy, os, threading

from rclpy.node import Node
from my_interfaces.srv import Command


DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
PROMPT = """
    prompt: Hello Deepseek! You will receive a text command from a robot. You need to convert it to a json string."
    prompt: find the plastic bottle.
        answer:{
            "commands":[
                {
                    "command": "find",
                    "parms":{
                        "goal":"red boll",
                    }
                },
            ]
        }
    prompt: find the red boll.
        answer:{
            "commands":[
                {
                    "command": "find",
                    "parms":{
                        "goal":"red boll",
                    }
                },
            ]
        }
    prompt: find the phone.
        answer:{
            "commands":[
                {
                    "command": "find",
                    "parms":{
                        "goal":"phone",
                    }
                },
            ]
        }
"""

class NLPNode(Node):
    def __init__(self, name):
        # Initialize the node
        super().__init__(name)
        self.get_logger().info("Node {} has been created.".format(name))
        
        self.command_client_ = self.create_client(Command, "/nlp/nlp_cmd")
        self.request = Command.Request()

        # Initialize the gemini model
        self.deepseek_client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")

         # Start a thread to wait for user input
        self.input_thread = threading.Thread(target=self.wait_for_input)
        self.input_thread.daemon = True
        self.input_thread.start()
    
    def dps_genai(self, msg):
        """
        This function uses the gemini model to generate a response to a prompt.
        
        Args:
            prompt(str): The prompt to generate
        
        Return:
            response.text(str): Json string generated by Gemini
        """

        response = self.deepseek_client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {
                    "role": "system", 
                    "content": PROMPT,
                },
                {
                    "role": "user", 
                    "content": msg,
                },
            ],
            stream=False
        )
        
        return response.choices[0].message.content
    
    def wait_for_input(self):
        """
        This function waits for user input and sends the input as a service request.
        """

        # Wait for the command service to be available  
        while not self.command_client_.wait_for_service(timeout_sec=5.0):
            self.get_logger().info("Wait for service '/nlp/nlp_cmd' ")

        user_input = input("Enter a command: ")
        
        self.request.command = self.dps_genai(user_input)

        future = self.command_client_.call_async(self.request)
        future.add_done_callback(self.handle_response)

    def handle_response(self, future):
        """
        This function handles the response from the command service.
        """

        try:
            response = future.result()
            self.get_logger().info("Response: {}".format(response.is_success))
            if response.is_success:
                self.wait_for_input()
            else:
                self.get_logger().fatal("oops! something went wrong")
                self.wait_for_input()
        except Exception as e:
            self.get_logger().error("Service call failed {}".format(e))
        
         
def main(args=None):
    rclpy.init(args=args)
    nlp_node = NLPNode("llm_nlp")
    rclpy.spin(nlp_node)
    rclpy.shutdown()

if __name__ == "__main__":
    main()
